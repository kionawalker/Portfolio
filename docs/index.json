[{"authors":["admin"],"categories":null,"content":"1994年，鹿児島生まれ．福岡大学 電子情報工学科 荒牧研究室でロボティクス(SLAM，ROS)を専攻．その後，九州大学大学院 倉爪研究室へ進学し画像認識(Deep Learning)を学ぶ．九州大学在学時は人の容姿から人物認証を行う歩容認証の研究に取り組み，Deep Learningを利用して解像度変化に頑健な歩容認証手法を開発．現在は某家電メーカーで画像ノイズ除去をはじめとする画像信号処理や人物の注視領域推定の研究開発に従事している．\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kionawalker.github.io/Portfolio/author/naoki-setoguchi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/Portfolio/author/naoki-setoguchi/","section":"authors","summary":"1994年，鹿児島生まれ．福岡大学 電子情報工学科 荒牧研究室で","tags":null,"title":"Naoki Setoguchi","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://kionawalker.github.io/Portfolio/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/Portfolio/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://kionawalker.github.io/Portfolio/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/Portfolio/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://kionawalker.github.io/Portfolio/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/Portfolio/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://kionawalker.github.io/Portfolio/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/Portfolio/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["Salient object detection"],"content":"arxivへのリンク paper\n掲載した画像は全て原著論文からの引用\nどんなもの？ 画像内の注目物体領域推定(SOD：Salient object detection)では，Saliency mapの推定にU-Net，Hourglass Net，FPNなどのU型ネットワークがよく用いられる．それらはEncoderとDecoderをSkip-connectionで接続するが，Encoderからの出力には冗長な情報も含まれるため学習や推論を阻害することが考えられる．例えば，SODの場合，背景と正解領域が似た場合に領域分割精度が低下する傾向にある．\nそこで，バイパス部分にEncoderの出力とDecoderの出力から決定されるGateを導入したGateNetを提案．またマルチスケールの特徴を得るASPPを改良したFold-ASPPを提案．\nDUTSなど主要な5つのデータセットでSoTA．\n先行研究と比べてどこがすごい？ 従来のU型ネットワークのSkip-connectionは，冗長な情報をDecoderに出力することを指摘し，選択的に処理するGateを導入したこと．また，Gateの情報をうまく利用してRefineするDual branch構造を提案したこと．\n技術や手法のキモはどこ？ GateNet 図2のまんま．\n各Encoderの出力とDecoderの出力をチャンネル方向にConcatする．その特徴からConvolutionとSigmoidでAttention mapを生成し，Encoderの出力を変換するTransition layerの出力と要素毎の積をとることでEncoderの出力のうち重要なものを選択する．得られた特徴はDecoderの出力に足されるFPN branchと推論のRefineを行うparallel branchに分かれる．\nGated dual branch  FPN branch\n画像のコンテキスト特徴を主に処理することで，注目物体が画像中のどの物体か推定する役割をもつ． Parallel branch\n物体の構造的特徴を得ることで，領域分割精度を高精度化する役割をもつ．各Gateからの出力はUpsamplingしてからFPN branchの出力とconcatする．その後，その特徴をConvolutionで処理した後，FPN branchと足し合わせてRefineする．  図４は各データセットで，各Gateの重みの平均をとったものであるが，FPN branchではより高次元な特徴に，Parallel branchではより低次元な特徴に大きな重みが与えられていることが分かる．\nFold-ASPP マルチスケールのDilated Convolutionによって広範囲の特徴抽出を行うASPPはDilation lateが大きくなると，特徴抽出するサンプルが異なりすぎてしまい，特徴抽出が安定しない．\nそこで，空間方向の2x2のパッチをチャンネル方向に並び変えて(=\u0026quot;Fold\u0026quot;と呼ぶ)からDilatated Convolutionすることで，実質2x2の範囲をサンプリングすることとなり安定して特徴抽出できる．\nどうやって有効だと検証した？ 推定精度 DUTSをはじめとする主要データセットで5つで検証． ほぼSoTAを総なめ．推論結果をみても背景が似通ったような画像で領域分割精度が向上していることが分かる．\nGateの導入効果 Gateなし/Gateありの各Decoderの出力を可視化したとき，Gateありの場合の方がより正解に近い領域を出力できている．\nFold ASPPの導入効果 Dilated Convolution，ASPPにおいて，Fold-ASPPで提案した\u0026quot;Fold\u0026quot;処理を行う効果を検証．全てにおいてFold-ASPPが有効に働いている．\n各モジュールの導入効果 Gate，Fold-ASPP，Parallel-branchを順に加えることで，徐々に精度が高まることを確認．GateとFold-ASPPの寄与が大きい．\n議論はある？ Gateを導入したU型ネットワークはDenseな推定を必要とするタスクに適している．\n次に読むべき論文は？ Pang, Y., Zhao, X., Zhang, L., Lu, H.: Multi-scale interactive network for salient object detection. In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. pp. 9413–9422 (2020)\n","date":1598792192,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598792192,"objectID":"e875590a85251e894b089e7dd0ac091c","permalink":"https://kionawalker.github.io/Portfolio/post/gatenet/","publishdate":"2020-08-30T21:56:32+09:00","relpermalink":"/Portfolio/post/gatenet/","section":"post","summary":"U型ネットワークのバイパス部にGateを設けたGateNetと，マルチスケール特徴を安定して抽出するFold-ASPPの提案","tags":["Salient object detection","ECCV2020"],"title":"Suppress and Balance: A Simple Gated Network for Salient Object Detection","type":"post"},{"authors":null,"categories":["Segmentation"],"content":"arxivへのリンク paper\n掲載した画像は全て原著論文からの引用\nどんなもの？ セグメンテーションのための深層学習手法は学習した解像度と異なる入力に対して適用したとき分割精度が低下する．\nまた，4Kを超えるような高解像度画像に対してセグメンテーションを行いたい場合，既存手法はGPUのメモリ量に限界があるため適用できないことがある．\nそこで，入力画像と3つの粗いセグメンテーション画像を用いて，新たにデータセットを学習させる必要なく，セグメンテーションのRefineを行うCascadePSPを提案．\n1枚画像を入力するGlobal Stepで，オブジェクトの構造をとらえ，切り抜き画像を入力とするLocal Stepで細部を処理することで，解像度の依存なくセグメンテーション結果のRefinementが可能．\n先行研究と比べてどこがすごい？ データセット，画像解像度に依存することなく，セグメンテーション結果のRefinementができること．Global StepとLocal Stepの合わせ技でGPUメモリの制限に対処できることも魅力．評価のために4K解像度のデータセットも新たに用意し，セグメンテーション結果の構造的な正確性を評価可能なmean Boundary Accuracy measure(mBA)も導入．\n技術や手法のキモはどこ？ Refinement Module(RM) 入力画像，既存のセグメンテーション手法で事前に得た解像度の異なる3つの粗いセグメンテーション結果を入力してResidual Blockで特徴抽出，その後，カーネルサイズが[1,2,4,6]のPiramid poolingでマルチスケールの特徴を捉える．この時，[1,2,4,8]のストライドで出力を得る．これらに対してUpsamplingで入力に用いたセグメンテーション結果と解像度を揃えてからConcatし，2層の1x1 ConvでRefineする．\nLoss 各スケールのRifine結果に対して異なる損失を定義する\n OS8\n物体全体の構造を捉えることが重要であるためCross Entropyを用いる OS1\n物体の細部を正確に捉えることが重要であるため，L1+L2 Lossを用いる OS4\n中間的な情報であるため，Cross Entropy + L1+L2  加えてエッジの正確さを保つため，Sobelフィルタで輝度勾配を計算したgradient lossを導入する\nGlobal Step 画像全体をRMに入力して全体の構造情報を考慮してRefineする．これをPiramid moduleの出力に対応してカスケード式に繰り返す．このとき元画像が4Kなどの高解像度画像だった場合，使用するGPUに応じて入力画像をダウンサンプリングする．\nLocal Step 画像をクロップしてそれをRMに入力し，細部をRefineする． このとき，画像境界のアーティファクトを避けるためにクロップの両端16pixを削ったり，ほとんどの画素が4つのクロップをカバーするようにストライドを決める．(意味が理解できていない)\nどうやって有効だと検証した？ Cascade構造とLoss設計の有効性 複数の出力を利用したCascade構造，Lossの組み合わせによる効果を示している．\nザリガニの例では1階の単一処理よりも3階のCascade処理のほうが，精細にRefineできていることがわかる．\nGlobal StepとLocal Stepの有効性 両者ともにRefinementの効果はある．Local stepのみを適用した場合にGlobal Stepもよりも精度が悪いのは，物体の構造のRefineができていないため．(粗いRefine ⇒ 細かいRefineが効果的)\n３つのデータセットで効果検証  セグメンテーション\nPASCAL VOC 2012とBIG(Original)\nBIGは4K解像度を含む高解像度画像のアノテーションが施された筆者独自のデータセット．解像度は2048×1600 ~ 5000×3600. 入力解像度の依存なくRefineできている．  しかし，失敗するケースもあり．\n Scene parsing\n\u0026ldquo;物体らしさ\u0026quot;の情報のみでRefineできるので，シーン解析(Semantic Segmentation）の高精度化にも拡張できる．\n画像全体に対してSemantic segmentationを行い，物体毎にCascadePSPを適用し，それらの結果をフュージョンする．(Divide-and-conquer strategy)  議論はある？ 入力さらたセグメンテーション画像を元にRifineするため，セグメンテーションが大きく失敗しているようなものではうまくいかない．\nしかし，実用的でSelf-supervisedなどのRefinement moduleとして活用することが見込める．\n次に読むべき論文は？ Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen. Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In CVPR, 2019. 2\nChao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters–improve semantic segmentation by global convolutional network. In CVPR, 2017.\nNing Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas Huang. Deep grabcut for object selection. In BMVC, 2017.\n","date":1593956043,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593956043,"objectID":"f7b2a7dc830e71c2bcf87051a75a9416","permalink":"https://kionawalker.github.io/Portfolio/post/cascadepsp/","publishdate":"2020-07-05T22:34:03+09:00","relpermalink":"/Portfolio/post/cascadepsp/","section":"post","summary":"新たなデータ・セットの学習なし，精度における入力データの解像度の依存なしでセグメンテーション結果をRefineできるCascadePSPを提案","tags":["Segmentation","Refinement","CVPR2020"],"title":"CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement","type":"post"},{"authors":null,"categories":["Object detection"],"content":"arxivへのリンク paper\n掲載した画像は全て原著論文からの引用\nどんなもの？ 深層学習はデータセットの規模に大きく影響を受ける．しかしOpen Imagesのような大規模データ・セットはアノテーターの認知能力の差異に基づくアノテーションミス(i.e. おもちゃの車：おもちゃ，車)や，カテゴリー間の極端なデータの不均衡が起こり得る．\nそこで，誤って付与された複数の正解ラベルに対して影響を受けないようにSoftmax関数を修正することでアノテーションミスに対応した． 更に，バランスの調整具合を決める任意パラメータを導入した提案手法のSoft-balance sampling，カテゴリー内のサンプル数によって調整される従来手法，このどちらを適用するかを学習時にスケジューリングすることでデータの不均衡に対応．\n先行研究と比べてどこがすごい？ 従来のOpen Imagesを利用した物体検出の学習では，不正確なラベルが複数つけられていたとしても考慮せず，単一のラベルのみを用いて学習を行っていた．(詳しくは分からない)\n提案手法では複数の正解ラベルを考慮してscoreを計算，損失を求めるため，誤った学習を行うことが抑制される．\nまた，データ不均衡に対して，カテゴリー内のサンプル数に応じてサンプリングを変える従来手法は，データセットの大部分が参照されなかったり，一部のデータだけ参照されすぎることが考えられ，学習を非効率的にする可能性があったが，soft-balance samplingとハイブリッドスケジューリングによって影響を緩和できる．\n技術や手法のキモはどこ？ Concurrent Softmax (1 - yi)項：正解ラベルが複数あるとき，モデルの出力したクラスの応答と対応したカテゴリーしか考慮しない．\n(i番目の応答ならばj=i以外に正解ラベルがあっても考慮しない．)\n(1 - rij)項：不正解ラベルは，誤ったラベルである割合を考慮して調整．rijは全ラベルの分布から求める．\nSoft-balance Sampling with Hybrid Training サンプリング方法として\n 学習するカテゴリーの総画像枚数/ 総画像枚数 = Pn 1 / 全てのカテゴリー数 = 1 / C\nの２つが考えられる．\nこれをパラメータλで操作できるようにしたものがSoft-balance sampling 学習では各epock毎にλを設定することで頻出しないカテゴリーの学習を進めたり，反対に極端に過学習しないように 調整する．  どうやって有効だと検証した？ 各手法について，個別に検証． 議論はある？ 特になし CVPR2020 Oralだけあって，とてもシンプルながら実応用に向いている．\n複数ラベルが混在してしまうのが，Open Imagesの特性なのか，一般的にそうなる傾向があるのか把握できていない．\n次に読むべき論文は？ 物体検出の基礎のFast R-CNNからさらっていく\n","date":1590364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590364800,"objectID":"793fdf15d263d464f5b7b4a03399172e","permalink":"https://kionawalker.github.io/Portfolio/post/od_iml/","publishdate":"2020-05-25T00:00:00Z","relpermalink":"/Portfolio/post/od_iml/","section":"post","summary":"大規模画像データ・セットがもつラベルのミスや不均衡の問題に対して，Softmax関数の修正と新たなサンプリング戦略で対応","tags":["Imbalanced labels","Open Images Dataset","CVPR2020"],"title":"Large-Scale Object Detection in the Wild from Imbalanced Multi-Labels","type":"post"},{"authors":null,"categories":["Person Re-ID"],"content":"arxivへのリンク paper\n掲載した画像は全て原著論文からの引用\nどんなもの？ Attention機構は特徴ベクトルの重要度を決めるとき，Targetの特徴のみで重要度を計算するLocal-Attentionと，全ての特徴を用いてTargetの重要度を計算するGlobal-Attention(Non-local Attention)に大別できる．\n前者は受容野に制限があるため特徴マップの大局的な情報(i.e. 腕\u0026quot;全体\u0026rdquo;)を考慮できず，後者は大局的な情報を考慮できるが位置情報が欠落するため，物体構造の関係性(i.e. Tシャツと靴の組み合わせ)を考慮できない.\nそこで，重要度を決めるTargetの特徴ベクトルと，その他の特徴ベクトルの関係性を計算するRelation-aware Global Attention(RGA)を提案．Person Re-IDと相性がよく3つのデータ・セットにおいてSoTA．\n先行研究と比べてどこがすごい？ 7x7の大きなカーネルを用いるAttention機構であるCBAMでも捉えることができない構造関係(i.e. 右手：ペットボトル，左手：手ぶら)を踏まえて特徴抽出が可能． また，Global Attentionは特徴マップ間でAttentionに差が生まれにくいが，RGAの特徴マップは多様になる．\n技術や手法のキモはどこ？ 特徴間の関係性を捉えるために，重要度を決めるTargetとなる特徴とその他の特徴のペアを変えてRelation特徴を算出する．その後Targetの特徴とRelation特徴の結合し，Targetの重要度を重み付けする.\nSpatial AttentionであるRGA-S，Channel AttentionであるRGA-Cを提案．\nRGA-SはCxHxWの特徴マップのうち，チャンネル方向の特徴をひとまとめとして扱い，N=0\u0026hellip;(HxW)までのindexとする．基準となるi番目の要素(1x1xC)とj番目の要素(1x1xC)をそれぞれ，1x1 Conv(θ，Φ)で変換(1x1x{C/r})して内積を計算する．\nこの処理を全てのi,jの組み合わせで行いNxNのAffinity matrixをつくる．\n参考：Affinity_matrix\n生成したAffinity matrixを行，列毎に分解しHxWのRelation特徴とする．\nRelation特徴と変換前の特徴はドメインが異なる情報をもつため，Global Average Poolingと1x1 Convで変換し次元削減する．その後にそれらを結合し，1x1 Conv(W1,W1)によってAttntionを生成する．\n同様にRGA-Cも計算できる．\nどうやって有効だと検証した？ RGA-S，RGA-CをResNet50のResidual Blockの後に挿入し効果を検証．\nRGA-SとRGA-Cのどちらも使う場合は順番に適用(RGA-CS, RGA-SC)，並列に処理(RGA-S//C)を検証． Local,Global Attentionと比較．構造の関係を考慮したRGAが凌駕．\nGlobal Attention(Non-local)はTarget(赤点)の位置が変わっても強く重みづけされる領域が変化しないが，RGA-Sは服，持ち物など関係のある領域に強く重みづけされる．\nLocal Attentionと比較すると保持するパラメータ数は増えるが，Global Attentionよりは少なくていい．\nAffinity matrixを生成するとき1x1 Convでembeddingする効果があるか検証．\n別々の重みをもつ2つの1x1 Convを利用したほうがよい(Asymmetric) (誤解釈があるかもしれない)\nSoTAと比較．2019年に提案された手法を大きく凌駕．\nGrad-CAMにより認識に寄与した領域を可視化すると，人の直感に沿った領域に強く反応していることがわかる．\n議論はある？ ResNetの場合，各ブロックにRGA-SCに挿入したほうがよい．\n次に読むべき論文は？ Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-excitation networks and beyond. arXiv preprint arXiv:1904.11492, 2019.\n","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589500800,"objectID":"a976e9c80fea73d58b0ab3e014ab1a77","permalink":"https://kionawalker.github.io/Portfolio/post/rga/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/Portfolio/post/rga/","section":"post","summary":"物体構造の関係性を考慮して重み付けするRelation-aware Global Attentionを提案","tags":["Attention","CVPR2020"],"title":"Relation-Aware Global Attention for Person Re-identification","type":"post"},{"authors":null,"categories":["Denoising"],"content":"arxivへのリンク paper\n掲載した画像は全て原著論文からの引用\nどんなもの？ 今まではsRGB画像でノイズ除去が行われていたが，sRGB画像はホワイトバランスやガンマ補正などの非線形変換を行うため，ノイズ分布が複雑になり推定が難しい．近年はデモザイクする前のRAWデータをノイズ除去に用いることが注目され，研究が活発化している．\nRAWデータを用いたデータ・セットはいくつか提案されているが，ノイズ画像とクリーン画像のペア作るには長露出で撮影する必要があり，動体がある状況では撮影が難しい．\nそこでおもちゃを利用し物体の動きを再現した新たなRAWデータ・セットを作成．いくつかの従来手法を取り入れたRViDeNetを構築し，従来のSoTAの手法と独自データ・セットで比較し最も良い精度を達成．\n先行研究と比べてどこがすごい？ 様々な手法を組み合わせてSoTAの精度を達成していること．\nデータ・セットにおもちゃを利用して作成した点はおもしろいが，動画のためのRAWデータの撮影システムは提案されており，やや新規性は弱まる．（論文ではおそらく意図的に参照されていない．)\n技術や手法のキモはどこ？ U-Netを用いたPre-denoisingの後に隣接フレームを利用した3層のPiramid deformable Conv， non-local attention機構を利用し，pre-denoisingによって平滑化されたエッジの推定を高度化していること\nAlignment ノイズ除去を行いたいフレームtの一つ前のフレームt+1の，pre-denoiseデータとnoiseデータを利用してConv層で特徴抽出する．\nその後，畳み込みフィルタの幾何的な制約を緩和するDeformable convを3層のpiramid構造をカスケード接続して適用することで広範囲の受容野をカバーしたピクセルのアライメントを行う．\nnon-local attention Spatial，Channel，Temporalの方向に1つのピクセルに対して，全てのピクセルを考慮して重み付けするnon-local attention機構を適応する．これによって，更に広範囲のアライメントが可能になる．\nTemporal fusionとSpatial fusion 従来手法であるelement-wise temporal fusion strategyで時間方向の統合(出力は1xCxHxH)を行い，10層のResNetにCBAMという特徴表現拡張機構を導入したネットワークで空間方向の統合を行う(出力は4xHxW)．\nどうやって有効だと検証した？ 独自データ・セットを用いてSoTAの手法と比較．ViDeNNは学習コードが再現できなかったため，公開された学習済みモデルを直接利用．\n各ステージを段階的に加えた場合の性能変化を比較．徐々に性能が向上している．\nノイズ除去画像の比較\n比較的エッジが残せている 議論はある？ 特になし．\n高精度化するために徐々にネットワークの構造が複雑化し，その分計算量も増えるため，処理速度を考慮した高速な手法が今後求められる．\n次に読むべき論文は？   Matias Tassano, Julie Delon, and Thomas Veit. Fastdvdnet: Towards real-time video denoising without explicit motion estimation. arXiv preprint arXiv:1907.01361, 2019.\n  Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 764–773, 2017.\n  Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3146–3154, 2019.\n  Haiyang Jiang and Yinqiang Zheng. Learning to see moving objects in the dark. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n  ","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"2d32151b3e2b10378d41a6d88a448abf","permalink":"https://kionawalker.github.io/Portfolio/post/rvidenet/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/Portfolio/post/rvidenet/","section":"post","summary":"おもちゃを使ったRaw video datasetを利用したRaw video denoising network(RViDeNet)の提案","tags":["Video Denoising","Dataset"],"title":"Supervised Raw Video Denoising with a Benchmark Dataset on Dynamic Scenes","type":"post"},{"authors":null,"categories":["Person Re-ID"],"content":"arxivへのリンク paper\n掲載した画像は全て原著論文からの引用\nどんなもの？ Person Re-IDのような人物照合タスクは照合データ間で照明，解像度，歩行方向，天候などの変化が生じる場合が多く，これらは認識精度の低下を招く．そこで，そのような変化の影響を受けないような人物特徴(Content特徴)と照合データ間の変動特徴(Degradation特徴)をDisentanglementするようにGANのEncoderを学習する(DDGAN)．次に学習した2つのEncoderとAttention機構を用いて特徴抽出器(DFEN)を学習することで，3つのデータ・セットを用いたCross-resolutionのPerson Re-IDでSoTAを達成．\n先行研究と比べてどこがすごい？ Person Re-IDにDisentangle learningを応用し，認識に必要な人物特徴と悪影響を与える特徴を分離することを可能にしており汎用性が高い．\n技術や手法のキモはどこ？ 入力として，通常画像Xi，通常画像を人工的に劣化させた劣化画像Xj，Xiとは別の人物の通常画像Xkを用いて，人物特徴(コンテンツ特徴)をエンコードするEc，劣化特徴を学習するEdとEd\u0026rsquo;をGANによる画像生成タスクで学習させる(パラメータは共有)．その際，\n Xiのコンテンツ特徴 ⇔ Xjのコンテンツ特徴 Xiの劣化特徴 ⇔ Xkの劣化特徴  をスワップすることで，各エンコーダーが入力の変化の影響を受けない特徴を獲得する(Disentangle)するように制約をかけている．更にDDGANで画像を生成する際，抽出した特徴の組み合わせを変えて画像を生成し，それらを比較する9つの損失関数を導入することで，コンテンツ特徴と劣化特徴の抽出を高精度化しつつ，ドメインGAPが生じないようにしている．\n最後はXi，Xj，Xkに対してEcとEdで抽出した特徴から生成した画像を用いて，劣化の影響を受けにくい特徴抽出器Eidの学習を行う．\nPerson Re-IDのための特徴抽出器(DFEN)の学習では，DDGANで学習したEcとEidを用いて学習を行う．そのとき，Ecは入力画像の変化に依存しないこと，Eidは入力の劣化具合の変化に応じて感度を変えて特徴抽出することが望ましい．そこで，Eidに対してAttention機構を導入することで調整を可能にしている．\nどうやって有効だと検証した？  3つのデータ・セットで性能検証しSoTA   DDGANのEcとResNetの中間層の特徴マップを比較して，提案手法で抽出した特徴は劣化の影響を受けにくいことを確認   コンテンツ画像から抽出したコンテンツ特徴とリファレンス画像から抽出した劣化特徴を用いて画像生成すると，コンテンツを維持したまま，解像度や照明を変化させた画像を生成でき，Disentangleできていることを確認   劣化特徴をt-SNEで2次元平面にプロットすると，劣化に関する教師なし学習であるのにも関わらず，照明具合が分離できていることを確認  議論はある？ 論文中には特になし\n以下が今後の課題と予想\n 複数の劣化成分への対応 コンテンツ特徴の高精度化 or 不変的な特徴の選択(骨格や歩行特徴など)  次に読むべき論文は？ Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, and Yu-Chiang Frank Wang. Recover and identify: A generative dual model for cross-resolution person re-identification. In The IEEE International Conference on Computer Vision (ICCV), October 2019\n","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588896000,"objectID":"3a158799b5294a9c6e97770fdc1883df","permalink":"https://kionawalker.github.io/Portfolio/post/ddgan/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/Portfolio/post/ddgan/","section":"post","summary":"Person Re-IDの弊害となる環境変化に対してDisentangle learningを取り入れたGANでEncoderを学習することで対応","tags":["Disentanglement","CVPR2020"],"title":"Real-world Person Re-Identification via Degradation Invariance Learning","type":"post"},{"authors":["Naoki Setoguchi"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://kionawalker.github.io/Portfolio/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/Portfolio/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://kionawalker.github.io/Portfolio/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/Portfolio/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://kionawalker.github.io/Portfolio/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/Portfolio/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://kionawalker.github.io/Portfolio/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/Portfolio/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Naoki Setoguchi","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://kionawalker.github.io/Portfolio/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/Portfolio/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Naoki Setoguchi","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://kionawalker.github.io/Portfolio/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/Portfolio/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CVPR2020 | Portfolio</title>
    <link>https://kionawalker.github.io/Portfolio/tag/cvpr2020/</link>
      <atom:link href="https://kionawalker.github.io/Portfolio/tag/cvpr2020/index.xml" rel="self" type="application/rss+xml" />
    <description>CVPR2020</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>setoro.naoki@gmail.com</copyright><lastBuildDate>Sun, 05 Jul 2020 22:34:03 +0900</lastBuildDate>
    <image>
      <url>https://kionawalker.github.io/Portfolio/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>CVPR2020</title>
      <link>https://kionawalker.github.io/Portfolio/tag/cvpr2020/</link>
    </image>
    
    <item>
      <title>CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement</title>
      <link>https://kionawalker.github.io/Portfolio/post/cascadepsp/</link>
      <pubDate>Sun, 05 Jul 2020 22:34:03 +0900</pubDate>
      <guid>https://kionawalker.github.io/Portfolio/post/cascadepsp/</guid>
      <description>&lt;p&gt;arxivへのリンク  
&lt;a href=&#34;https://arxiv.org/abs/2005.02551&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;br&gt;
掲載した画像は全て原著論文からの引用&lt;/p&gt;
&lt;h2 id=&#34;どんなもの&#34;&gt;&lt;strong&gt;どんなもの？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;セグメンテーションのための深層学習手法は学習した解像度と異なる入力に対して適用したとき分割精度が低下する．&lt;br&gt;
また，4Kを超えるような高解像度画像に対してセグメンテーションを行いたい場合，既存手法はGPUのメモリ量に限界があるため適用できないことがある．&lt;br&gt;
そこで，入力画像と3つの粗いセグメンテーション画像を用いて，新たにデータセットを学習させる必要なく，セグメンテーションのRefineを行うCascadePSPを提案．&lt;br&gt;
1枚画像を入力するGlobal Stepで，オブジェクトの構造をとらえ，切り抜き画像を入力とするLocal Stepで細部を処理することで，解像度の依存なくセグメンテーション結果のRefinementが可能．&lt;/p&gt;
&lt;h2 id=&#34;先行研究と比べてどこがすごい&#34;&gt;&lt;strong&gt;先行研究と比べてどこがすごい？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;データセット，画像解像度に依存することなく，セグメンテーション結果のRefinementができること．Global StepとLocal Stepの合わせ技でGPUメモリの制限に対処できることも魅力．評価のために4K解像度のデータセットも新たに用意し，セグメンテーション結果の構造的な正確性を評価可能なmean Boundary Accuracy measure(mBA)も導入．&lt;/p&gt;
&lt;h2 id=&#34;技術や手法のキモはどこ&#34;&gt;&lt;strong&gt;技術や手法のキモはどこ？&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;refinement-modulerm&#34;&gt;Refinement Module(RM)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;rm.png&#34; alt=&#34;RM&#34;&gt;
入力画像，既存のセグメンテーション手法で事前に得た解像度の異なる3つの粗いセグメンテーション結果を入力してResidual Blockで特徴抽出，その後，カーネルサイズが[1,2,4,6]のPiramid poolingでマルチスケールの特徴を捉える．この時，[1,2,4,8]のストライドで出力を得る．これらに対してUpsamplingで入力に用いたセグメンテーション結果と解像度を揃えてからConcatし，2層の1x1 ConvでRefineする．&lt;/p&gt;
&lt;h3 id=&#34;loss&#34;&gt;Loss&lt;/h3&gt;
&lt;p&gt;各スケールのRifine結果に対して異なる損失を定義する&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OS8&lt;br&gt;
　物体全体の構造を捉えることが重要であるためCross Entropyを用いる&lt;/li&gt;
&lt;li&gt;OS1&lt;br&gt;
物体の細部を正確に捉えることが重要であるため，L1+L2 Lossを用いる&lt;/li&gt;
&lt;li&gt;OS4&lt;br&gt;
中間的な情報であるため，Cross Entropy + L1+L2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;加えてエッジの正確さを保つため，Sobelフィルタで輝度勾配を計算したgradient lossを導入する&lt;/p&gt;
&lt;h3 id=&#34;global-step&#34;&gt;Global Step&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;gs.png&#34; alt=&#34;GS&#34;&gt;&lt;br&gt;
画像全体をRMに入力して全体の構造情報を考慮してRefineする．これをPiramid moduleの出力に対応してカスケード式に繰り返す．このとき元画像が4Kなどの高解像度画像だった場合，使用するGPUに応じて入力画像をダウンサンプリングする．&lt;/p&gt;
&lt;h3 id=&#34;local-step&#34;&gt;Local Step&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;ls.png&#34; alt=&#34;LS&#34;&gt;&lt;br&gt;
画像をクロップしてそれをRMに入力し，細部をRefineする．
このとき，画像境界のアーティファクトを避けるためにクロップの両端16pixを削ったり，ほとんどの画素が4つのクロップをカバーするようにストライドを決める．(意味が理解できていない)&lt;/p&gt;
&lt;h2 id=&#34;どうやって有効だと検証した&#34;&gt;&lt;strong&gt;どうやって有効だと検証した？&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;cascade構造とloss設計の有効性&#34;&gt;Cascade構造とLoss設計の有効性&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;table1.png&#34; alt=&#34;table1&#34;&gt;
&lt;img src=&#34;fig3.png&#34; alt=&#34;fig3&#34;&gt;&lt;br&gt;
複数の出力を利用したCascade構造，Lossの組み合わせによる効果を示している．&lt;br&gt;
ザリガニの例では1階の単一処理よりも3階のCascade処理のほうが，精細にRefineできていることがわかる．&lt;/p&gt;
&lt;h3 id=&#34;global-stepとlocal-stepの有効性&#34;&gt;Global StepとLocal Stepの有効性&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;table2.png&#34; alt=&#34;table2&#34;&gt;&lt;br&gt;
両者ともにRefinementの効果はある．Local stepのみを適用した場合にGlobal Stepもよりも精度が悪いのは，物体の構造のRefineができていないため．(粗いRefine ⇒ 細かいRefineが効果的)&lt;/p&gt;
&lt;h3 id=&#34;３つのデータセットで効果検証&#34;&gt;３つのデータセットで効果検証&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;セグメンテーション&lt;br&gt;
&lt;img src=&#34;fig10.png&#34; alt=&#34;fig10&#34;&gt;&lt;br&gt;
&lt;strong&gt;PASCAL VOC 2012とBIG(Original)&lt;/strong&gt;&lt;br&gt;
BIGは4K解像度を含む高解像度画像のアノテーションが施された筆者独自のデータセット．解像度は2048×1600 ~ 5000×3600.
入力解像度の依存なくRefineできている．&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;fig11.png&#34; alt=&#34;fig11&#34;&gt;&lt;br&gt;
しかし，失敗するケースもあり．&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scene parsing&lt;br&gt;
&lt;img src=&#34;fig12.png&#34; alt=&#34;fig12&#34;&gt;&lt;br&gt;
&lt;img src=&#34;fig13.png&#34; alt=&#34;fig13&#34;&gt;&lt;br&gt;
&amp;ldquo;物体らしさ&amp;quot;の情報のみでRefineできるので，シーン解析(Semantic Segmentation）の高精度化にも拡張できる．&lt;br&gt;
画像全体に対してSemantic segmentationを行い，物体毎にCascadePSPを適用し，それらの結果をフュージョンする．(Divide-and-conquer strategy)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;議論はある&#34;&gt;&lt;strong&gt;議論はある？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;入力さらたセグメンテーション画像を元にRifineするため，セグメンテーションが大きく失敗しているようなものではうまくいかない．&lt;br&gt;
しかし，実用的でSelf-supervisedなどのRefinement moduleとして活用することが見込める．&lt;/p&gt;
&lt;h2 id=&#34;次に読むべき論文は&#34;&gt;&lt;strong&gt;次に読むべき論文は？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua
Shen. Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In CVPR, 2019. 2&lt;/p&gt;
&lt;p&gt;Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and
Jian Sun. Large kernel matters–improve semantic segmentation by global convolutional network. In CVPR, 2017.&lt;/p&gt;
&lt;p&gt;Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas
Huang. Deep grabcut for object selection. In BMVC, 2017.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Large-Scale Object Detection in the Wild from Imbalanced Multi-Labels</title>
      <link>https://kionawalker.github.io/Portfolio/post/od_iml/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      <guid>https://kionawalker.github.io/Portfolio/post/od_iml/</guid>
      <description>&lt;p&gt;arxivへのリンク  
&lt;a href=&#34;https://arxiv.org/abs/2005.08455&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;br&gt;
掲載した画像は全て原著論文からの引用&lt;/p&gt;
&lt;h2 id=&#34;どんなもの&#34;&gt;&lt;strong&gt;どんなもの？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;深層学習はデータセットの規模に大きく影響を受ける．しかしOpen Imagesのような大規模データ・セットはアノテーターの認知能力の差異に基づくアノテーションミス(i.e. おもちゃの車：おもちゃ，車)や，カテゴリー間の極端なデータの不均衡が起こり得る．&lt;br&gt;
そこで，誤って付与された複数の正解ラベルに対して影響を受けないようにSoftmax関数を修正することでアノテーションミスに対応した．  更に，バランスの調整具合を決める任意パラメータを導入した提案手法のSoft-balance sampling，カテゴリー内のサンプル数によって調整される従来手法，このどちらを適用するかを学習時にスケジューリングすることでデータの不均衡に対応．&lt;/p&gt;
&lt;h2 id=&#34;先行研究と比べてどこがすごい&#34;&gt;&lt;strong&gt;先行研究と比べてどこがすごい？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;従来のOpen Imagesを利用した物体検出の学習では，不正確なラベルが複数つけられていたとしても考慮せず，単一のラベルのみを用いて学習を行っていた．(詳しくは分からない)&lt;br&gt;
提案手法では複数の正解ラベルを考慮してscoreを計算，損失を求めるため，誤った学習を行うことが抑制される．&lt;br&gt;
また，データ不均衡に対して，カテゴリー内のサンプル数に応じてサンプリングを変える従来手法は，データセットの大部分が参照されなかったり，一部のデータだけ参照されすぎることが考えられ，学習を非効率的にする可能性があったが，soft-balance samplingとハイブリッドスケジューリングによって影響を緩和できる．&lt;/p&gt;
&lt;h2 id=&#34;技術や手法のキモはどこ&#34;&gt;&lt;strong&gt;技術や手法のキモはどこ？&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;concurrent-softmax&#34;&gt;Concurrent Softmax&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;fig3.png&#34; alt=&#34;fig3&#34;&gt;
&lt;img src=&#34;softmax.png&#34; alt=&#34;softmax&#34;&gt;
(1 - yi)項：正解ラベルが複数あるとき，モデルの出力したクラスの応答と対応したカテゴリーしか考慮しない．&lt;br&gt;
(i番目の応答ならばj=i以外に正解ラベルがあっても考慮しない．)&lt;/p&gt;
&lt;p&gt;(1 - rij)項：不正解ラベルは，誤ったラベルである割合を考慮して調整．rijは全ラベルの分布から求める．&lt;/p&gt;
&lt;h3 id=&#34;soft-balance-sampling-with-hybrid-training&#34;&gt;Soft-balance Sampling with Hybrid Training&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;soft.png&#34; alt=&#34;sampling&#34;&gt;
サンプリング方法として&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学習するカテゴリーの総画像枚数/ 総画像枚数 = Pn&lt;/li&gt;
&lt;li&gt;1 / 全てのカテゴリー数 = 1 / C&lt;br&gt;
の２つが考えられる．&lt;br&gt;
これをパラメータλで操作できるようにしたものがSoft-balance sampling
学習では各epock毎にλを設定することで頻出しないカテゴリーの学習を進めたり，反対に極端に過学習しないように
調整する．&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;どうやって有効だと検証した&#34;&gt;&lt;strong&gt;どうやって有効だと検証した？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;各手法について，個別に検証．
&lt;img src=&#34;table3.png&#34; alt=&#34;table3&#34;&gt;
&lt;img src=&#34;fig4.png&#34; alt=&#34;fig4&#34;&gt;
&lt;img src=&#34;table4.png&#34; alt=&#34;table4&#34;&gt;
&lt;img src=&#34;table5.png&#34; alt=&#34;table5&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;議論はある&#34;&gt;&lt;strong&gt;議論はある？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;特になし
CVPR2020 Oralだけあって，とてもシンプルながら実応用に向いている．&lt;br&gt;
複数ラベルが混在してしまうのが，Open Imagesの特性なのか，一般的にそうなる傾向があるのか把握できていない．&lt;/p&gt;
&lt;h2 id=&#34;次に読むべき論文は&#34;&gt;&lt;strong&gt;次に読むべき論文は？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;物体検出の基礎のFast R-CNNからさらっていく&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Relation-Aware Global Attention for Person Re-identification</title>
      <link>https://kionawalker.github.io/Portfolio/post/rga/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <guid>https://kionawalker.github.io/Portfolio/post/rga/</guid>
      <description>&lt;p&gt;arxivへのリンク  
&lt;a href=&#34;https://arxiv.org/abs/1904.02998&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;br&gt;
掲載した画像は全て原著論文からの引用&lt;/p&gt;
&lt;h2 id=&#34;どんなもの&#34;&gt;&lt;strong&gt;どんなもの？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Attention機構は特徴ベクトルの重要度を決めるとき，Targetの特徴のみで重要度を計算するLocal-Attentionと，全ての特徴を用いてTargetの重要度を計算するGlobal-Attention(Non-local Attention)に大別できる．&lt;br&gt;
前者は受容野に制限があるため特徴マップの大局的な情報(i.e. 腕&amp;quot;全体&amp;rdquo;)を考慮できず，後者は大局的な情報を考慮できるが位置情報が欠落するため，物体構造の関係性(i.e. Tシャツと靴の組み合わせ)を考慮できない.&lt;br&gt;
そこで，重要度を決めるTargetの特徴ベクトルと，その他の特徴ベクトルの関係性を計算するRelation-aware Global Attention(RGA)を提案．Person Re-IDと相性がよく3つのデータ・セットにおいてSoTA．&lt;/p&gt;
&lt;h2 id=&#34;先行研究と比べてどこがすごい&#34;&gt;&lt;strong&gt;先行研究と比べてどこがすごい？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;7x7の大きなカーネルを用いるAttention機構であるCBAMでも捉えることができない構造関係(i.e. 右手：ペットボトル，左手：手ぶら)を踏まえて特徴抽出が可能．
また，Global Attentionは特徴マップ間でAttentionに差が生まれにくいが，RGAの特徴マップは多様になる．&lt;/p&gt;
&lt;h2 id=&#34;技術や手法のキモはどこ&#34;&gt;&lt;strong&gt;技術や手法のキモはどこ？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;concept.png&#34; alt=&#34;concept&#34;&gt;
特徴間の関係性を捉えるために，重要度を決めるTargetとなる特徴とその他の特徴のペアを変えてRelation特徴を算出する．その後Targetの特徴とRelation特徴の結合し，Targetの重要度を重み付けする.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;RGA.png&#34; alt=&#34;RGA&#34;&gt;
Spatial AttentionであるRGA-S，Channel AttentionであるRGA-Cを提案．&lt;br&gt;
RGA-SはCxHxWの特徴マップのうち，チャンネル方向の特徴をひとまとめとして扱い，N=0&amp;hellip;(HxW)までのindexとする．基準となるi番目の要素(1x1xC)とj番目の要素(1x1xC)をそれぞれ，1x1 Conv(θ，Φ)で変換(1x1x{C/r})して内積を計算する．&lt;br&gt;
この処理を全てのi,jの組み合わせで行いNxNのAffinity matrixをつくる．&lt;br&gt;

&lt;a href=&#34;https://qiita.com/pacifinapacific/items/2e7a03aa84d8a8bfa60d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;参考：Affinity_matrix&lt;/a&gt;&lt;br&gt;
生成したAffinity matrixを行，列毎に分解しHxWのRelation特徴とする．&lt;br&gt;
Relation特徴と変換前の特徴はドメインが異なる情報をもつため，Global Average Poolingと1x1 Convで変換し次元削減する．その後にそれらを結合し，1x1 Conv(W1,W1)によってAttntionを生成する．&lt;br&gt;
同様にRGA-Cも計算できる．&lt;/p&gt;
&lt;h2 id=&#34;どうやって有効だと検証した&#34;&gt;&lt;strong&gt;どうやって有効だと検証した？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;table1.png&#34; alt=&#34;Table1&#34;&gt;
RGA-S，RGA-CをResNet50のResidual Blockの後に挿入し効果を検証．&lt;br&gt;
RGA-SとRGA-Cのどちらも使う場合は順番に適用(RGA-CS, RGA-SC)，並列に処理(RGA-S//C)を検証．
　
&lt;img src=&#34;table2.png&#34; alt=&#34;Table2&#34;&gt;
Local,Global Attentionと比較．構造の関係を考慮したRGAが凌駕．&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig4.png&#34; alt=&#34;Fig4&#34;&gt;
Global Attention(Non-local)はTarget(赤点)の位置が変わっても強く重みづけされる領域が変化しないが，RGA-Sは服，持ち物など関係のある領域に強く重みづけされる．&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table3.png&#34; alt=&#34;Table3&#34;&gt;
Local Attentionと比較すると保持するパラメータ数は増えるが，Global Attentionよりは少なくていい．&lt;br&gt;
Affinity matrixを生成するとき1x1 Convでembeddingする効果があるか検証．&lt;br&gt;
別々の重みをもつ2つの1x1 Convを利用したほうがよい(Asymmetric)
(誤解釈があるかもしれない)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table5.png&#34; alt=&#34;Table5&#34;&gt;
SoTAと比較．2019年に提案された手法を大きく凌駕．&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig5.png&#34; alt=&#34;Fig5&#34;&gt;
Grad-CAMにより認識に寄与した領域を可視化すると，人の直感に沿った領域に強く反応していることがわかる．&lt;/p&gt;
&lt;h2 id=&#34;議論はある&#34;&gt;&lt;strong&gt;議論はある？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;ResNetの場合，各ブロックにRGA-SCに挿入したほうがよい．&lt;/p&gt;
&lt;h2 id=&#34;次に読むべき論文は&#34;&gt;&lt;strong&gt;次に読むべき論文は？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han
Hu. Gcnet: &lt;em&gt;Non-local networks meet squeeze-excitation networks and beyond.&lt;/em&gt; arXiv preprint arXiv:1904.11492, 2019.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-world Person Re-Identification via Degradation Invariance Learning</title>
      <link>https://kionawalker.github.io/Portfolio/post/ddgan/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>https://kionawalker.github.io/Portfolio/post/ddgan/</guid>
      <description>&lt;p&gt;arxivへのリンク  
&lt;a href=&#34;https://arxiv.org/abs/2004.04933&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;br&gt;
掲載した画像は全て原著論文からの引用&lt;/p&gt;
&lt;h2 id=&#34;どんなもの&#34;&gt;&lt;strong&gt;どんなもの？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Person Re-IDのような人物照合タスクは照合データ間で照明，解像度，歩行方向，天候などの変化が生じる場合が多く，これらは認識精度の低下を招く．そこで，そのような変化の影響を受けないような人物特徴(Content特徴)と照合データ間の変動特徴(Degradation特徴)をDisentanglementするようにGANのEncoderを学習する(DDGAN)．次に学習した2つのEncoderとAttention機構を用いて特徴抽出器(DFEN)を学習することで，3つのデータ・セットを用いたCross-resolutionのPerson Re-IDでSoTAを達成．&lt;/p&gt;
&lt;h2 id=&#34;先行研究と比べてどこがすごい&#34;&gt;&lt;strong&gt;先行研究と比べてどこがすごい？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Person Re-IDにDisentangle learningを応用し，認識に必要な人物特徴と悪影響を与える特徴を分離することを可能にしており汎用性が高い．&lt;/p&gt;
&lt;h2 id=&#34;技術や手法のキモはどこ&#34;&gt;&lt;strong&gt;技術や手法のキモはどこ？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;DDGAN.png&#34; alt=&#34;DDGAN&#34;&gt;
入力として，通常画像Xi，通常画像を人工的に劣化させた劣化画像Xj，Xiとは別の人物の通常画像Xkを用いて，人物特徴(コンテンツ特徴)をエンコードするEc，劣化特徴を学習するEdとEd&amp;rsquo;をGANによる画像生成タスクで学習させる(パラメータは共有)．その際，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Xiのコンテンツ特徴 ⇔ Xjのコンテンツ特徴&lt;/li&gt;
&lt;li&gt;Xiの劣化特徴 ⇔ Xkの劣化特徴&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;をスワップすることで，各エンコーダーが入力の変化の影響を受けない特徴を獲得する(Disentangle)するように制約をかけている．更にDDGANで画像を生成する際，抽出した特徴の組み合わせを変えて画像を生成し，それらを比較する9つの損失関数を導入することで，コンテンツ特徴と劣化特徴の抽出を高精度化しつつ，ドメインGAPが生じないようにしている．&lt;br&gt;
最後はXi，Xj，Xkに対してEcとEdで抽出した特徴から生成した画像を用いて，劣化の影響を受けにくい特徴抽出器Eidの学習を行う．&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;DFEN.png&#34; alt=&#34;DFEN&#34;&gt;
Person Re-IDのための特徴抽出器(DFEN)の学習では，DDGANで学習したEcとEidを用いて学習を行う．そのとき，Ecは入力画像の変化に依存しないこと，Eidは入力の劣化具合の変化に応じて感度を変えて特徴抽出することが望ましい．そこで，Eidに対してAttention機構を導入することで調整を可能にしている．&lt;/p&gt;
&lt;h2 id=&#34;どうやって有効だと検証した&#34;&gt;&lt;strong&gt;どうやって有効だと検証した？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;table1.png&#34; alt=&#34;Table&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3つのデータ・セットで性能検証しSoTA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;feature_map.png&#34; alt=&#34;Map&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DDGANのEcとResNetの中間層の特徴マップを比較して，提案手法で抽出した特徴は劣化の影響を受けにくいことを確認&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;res.png&#34; alt=&#34;Resolution&#34;&gt;
&lt;img src=&#34;ill.png&#34; alt=&#34;Illumination&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コンテンツ画像から抽出したコンテンツ特徴とリファレンス画像から抽出した劣化特徴を用いて画像生成すると，コンテンツを維持したまま，解像度や照明を変化させた画像を生成でき，Disentangleできていることを確認&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;t-SNE.png&#34; alt=&#34;T-SNE&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;劣化特徴をt-SNEで2次元平面にプロットすると，劣化に関する教師なし学習であるのにも関わらず，照明具合が分離できていることを確認&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;議論はある&#34;&gt;&lt;strong&gt;議論はある？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;論文中には特になし&lt;/p&gt;
&lt;p&gt;以下が今後の課題と予想&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;複数の劣化成分への対応&lt;/li&gt;
&lt;li&gt;コンテンツ特徴の高精度化 or 不変的な特徴の選択(骨格や歩行特徴など)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;次に読むべき論文は&#34;&gt;&lt;strong&gt;次に読むべき論文は？&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, and
Yu-Chiang Frank Wang. &lt;em&gt;Recover and identify: A generative dual model for cross-resolution person re-identification.&lt;/em&gt;
In The IEEE International Conference on Computer Vision
(ICCV), October 2019&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
